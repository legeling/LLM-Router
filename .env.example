# LLM-Router Environment Configuration
# Copy this file to .env and fill in your values

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=false
WORKERS=4

# API Authentication
API_KEY=your-secure-api-key-here
# Generate a secure key: python -c "import secrets; print(secrets.token_urlsafe(32))"

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_FILE=logs/llm_router.log

# Model Configuration
MODELS_CONFIG_PATH=config/models.json

# Cache Configuration (Optional)
REDIS_URL=redis://localhost:6379/0
CACHE_TTL=3600

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60

# Monitoring (Optional)
ENABLE_METRICS=true
METRICS_PORT=9090

# CORS Configuration
CORS_ORIGINS=*
# In production, specify allowed origins: CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# Timeout Configuration
REQUEST_TIMEOUT=30
PROVIDER_TIMEOUT=25

# Database (Optional, for usage tracking)
# DATABASE_URL=postgresql://user:password@localhost:5432/llm_router

# Sentry (Optional, for error tracking)
# SENTRY_DSN=https://your-sentry-dsn
